---
layout: homepage
---


<div class="row">
    <div class="col-md-7">
    <div class="block"></div>
      <p>Hi, I'm Meghana!</p>
      <style>
        .intro-link {
          color: #7c3aed;
          text-decoration: none;
          font-weight: 500;
        }
        .intro-link:hover {
          color: #6d28d9;
          text-decoration: underline;
        }
        .highlight-bold {
          font-weight: bold;
          color: #7c3aed;
        }
        .journal-title {
          font-weight: bold;
          color: #7c3aed;
        }
      </style>
      <p>I'm a B.Tech student in Computer Science and Engineering (Data Science) at <a href="https://chennai.vit.ac.in/" class="intro-link"><strong>VIT University</strong></a>, where I am advised by <a href="https://chennai.vit.ac.in/member/dr-joe-dhanith-p-r/" class="intro-link"><strong>Dr. Joe Dhanith</strong></a>. My research focuses on large multimodal models for reasoning, spanning representation learning, domain generalization, and cross-modal intelligence. I'm broadly interested in how multimodal systems learn structure across vision, language, and neural signals, and how these models become more robust, interpretable, and adaptable.</p>
      <p>Currently, I'm a Machine Learning Intern at <a href="https://en.nagasaki-u.ac.jp/" class="intro-link"><strong>Nagasaki University</strong></a> advised by <a href="https://scholar.google.com/citations?user=QgdtEqQAAAAJ&hl=en" class="intro-link"><strong>Dr. Muthu Subash Kavitha</strong></a>, where I work on improving implicit reasoning in large multimodal models. In parallel, I also work with <a href="https://www.cmu.edu/cbd/people/xu.html" class="intro-link"><strong>Dr. Min Xu</strong></a> at the Xu Lab at <a href="https://www.cmu.edu/" class="intro-link"><strong>Carnegie Mellon University</strong></a> on multimodal learning and medical AI, focusing on EEG–fNIRS fusion through state-space–based sequence modeling. Prior to this, I was a Machine Learning Intern at <a href="https://multicorewareinc.com/" class="intro-link"><strong>MulticoreWare Inc.</strong></a>, where I worked on model optimization and post-training quantization, and on finetuning large-scale transformers for efficient deployment. I also interned at the Center for Neuroinformatics with <a href="https://chennai.vit.ac.in/member/dr-jeetashree-aparajeeta/" class="intro-link"><strong>Dr. Jeetashree Aparajita</strong></a>, where I worked on multimodal EEG–fNIRS learning fusion pipelines for cognitive stress analysis. I am a recipient of the <span class="highlight-bold">Raman Research Award</span> at <a href="https://chennai.vit.ac.in/" class="intro-link"><strong>VIT</strong></a> for my research contributions.</p>
      <p>Outside of academics, I love dancing. I lead major cultural events on campus and serve as <span class="highlight-bold">President</span> of <a href="https://www.instagram.com/dance_club_vitc/?hl=en" class="intro-link"><strong>VITc's dance club</strong></a>, where I choreograph, perform, and build spaces for artistic expression.</p>

      <p><a href="mailto:meghanaa.sunil@gmail.com"><u>email</u></a> / <a href="https://www.linkedin.com/in/meghana-sunil"><u>linkedin</u></a> / <a href="https://github.com/meghanaasunil"><u>github</u></a> / <a href="https://scholar.google.com/citations?user=odpwYzQAAAAJ&hl=en"><u>google scholar</u></a></p>
    </div>
    <div class="col-md-5">
        <img src="/assets/img/me/Meghana_sunil.jpeg?v=2" alt="Meghana Sunil" >
    </div>
</div>
<div>&nbsp;</div>

<h2>Selected Works</h2>

<div class="row" style="margin-bottom: 30px;">
    <div class="col-md-4">
        <img src="/assets/img/research/contrastive-learning.jpg" alt="Contrastive Learning Architecture" style="max-width: 100%; height: auto;">
    </div>
    <div class="col-md-8">
        <h4><strong>Contrastive Learning Under Domain Shift: Do Positive Pairs Help or Hurt Robustness?</strong></h4>
        <p><strong>Meghana Sunil</strong></p>
        <p><i>In Progress</i></p>
        <p>Positive pairs aren't universally beneficial under domain shift—robustness increases only when their influence adapts to classifier confidence.</p>
        <p>[<a href="#">Paper</a>] [<a href="#">Code</a>]</p>
    </div>
</div>

<div class="row" style="margin-bottom: 30px;">
    <div class="col-md-4">
        <img src="/assets/img/research/eeg-fnirs.jpg" alt="EEG-fNIRS Architecture" style="max-width: 100%; height: auto;">
    </div>
    <div class="col-md-8">
        <h4><strong>Graph-Guided Cross-Modal Representation Learning of EEG–fNIRS via Spatio-Temporal Transformers for Stress Analysis</strong></h4>
        <p><strong>Meghana Sunil</strong></p>
        <p><i>Submitted: <span class="journal-title">Computer Methods and Programs in Biomedicine</span></i></p>
        <p>Cross-modal stress classification benefits from explicitly modeling EEG–fNIRS structural dependencies, where graph-guided fusion yields more reliable spatio-temporal representations.</p>
        <p>[<a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5564927">Paper</a>] [<a href="https://github.com/meghanaasunil/CM-GGT">Code</a>]</p>
    </div>
</div>

<div class="row" style="margin-bottom: 30px;">
    <div class="col-md-4">
        <img src="/assets/img/research/lung-cancer.jpg" alt="Lung Cancer Diagnosis Architecture" style="max-width: 100%; height: auto;">
    </div>
    <div class="col-md-8">
        <h4><strong>Encouraging Discriminative Attention Through Contrastive Explainability Learning for Lung Cancer Diagnosis</strong></h4>
        <p><strong>Meghana Sunil</strong></p>
        <p><i><span class="journal-title">IEEE Access</span></i></p>
        <p>Enforcing contrastive constraints on attention maps improves both classification performance and attribution consistency, showing that explanation-guided objectives can regulate feature selection.</p>
        <p>[<a href="https://ieeexplore.ieee.org/abstract/document/11184753">Paper</a>] [<a href="https://github.com/meghanaasunil/Contrastive-Explainability-Learning-for-Lung-Cancer-Diagnosis">Code</a>]</p>
    </div>
</div>

<p>For more research work, please visit my <a href="/research">Research</a> page.</p>

